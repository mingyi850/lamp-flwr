INFO flwr 2023-12-18 02:04:18,866 | app.py:163 | Starting Flower server, config: ServerConfig(num_rounds=100, round_timeout=None)
INFO flwr 2023-12-18 02:04:18,918 | app.py:176 | Flower ECE: gRPC server running (100 rounds), SSL is disabled
INFO flwr 2023-12-18 02:04:18,918 | server.py:89 | Initializing global parameters
INFO flwr 2023-12-18 02:04:18,918 | server.py:276 | Requesting initial parameters from one random client
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.1.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.2.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.weight', 'fit_denses.6.bias', 'fit_denses.3.bias', 'fit_denses.0.bias', 'fit_denses.1.weight', 'cls.seq_relationship.weight', 'fit_denses.4.weight', 'cls.predictions.decoder.weight', 'fit_denses.2.bias', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.5.bias', 'fit_denses.4.bias', 'cls.seq_relationship.bias', 'fit_denses.6.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.5.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'fit_denses.3.weight', 'fit_denses.1.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'fit_denses.6.bias', 'cls.predictions.decoder.weight', 'fit_denses.5.bias', 'fit_denses.4.bias', 'fit_denses.2.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.2.bias', 'fit_denses.1.weight', 'fit_denses.5.weight', 'cls.seq_relationship.bias', 'fit_denses.4.weight', 'fit_denses.3.bias', 'fit_denses.6.weight', 'fit_denses.0.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.weight', 'fit_denses.6.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.6.bias', 'fit_denses.5.weight', 'fit_denses.4.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.1.bias', 'fit_denses.0.bias', 'fit_denses.3.weight', 'fit_denses.2.bias', 'fit_denses.2.weight', 'fit_denses.4.bias', 'fit_denses.0.weight', 'fit_denses.3.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'fit_denses.5.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'fit_denses.6.bias', 'fit_denses.4.bias', 'fit_denses.0.bias', 'fit_denses.2.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.6.weight', 'fit_denses.4.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'fit_denses.0.weight', 'fit_denses.3.weight', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.5.weight', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.5.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'fit_denses.3.bias', 'fit_denses.1.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.5.weight', 'fit_denses.6.weight', 'cls.seq_relationship.weight', 'fit_denses.4.weight', 'fit_denses.1.weight', 'fit_denses.5.bias', 'fit_denses.3.bias', 'fit_denses.4.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'fit_denses.0.bias', 'cls.predictions.decoder.weight', 'fit_denses.2.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.0.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.6.bias', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.weight', 'cls.predictions.bias', 'fit_denses.1.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.0.bias', 'fit_denses.6.weight', 'fit_denses.6.bias', 'cls.seq_relationship.weight', 'fit_denses.5.weight', 'fit_denses.1.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.0.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.3.bias', 'fit_denses.2.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.4.bias', 'fit_denses.2.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.5.bias', 'fit_denses.4.weight', 'fit_denses.1.bias', 'fit_denses.3.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.4.weight', 'fit_denses.6.bias', 'fit_denses.5.bias', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.3.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.weight', 'fit_denses.2.weight', 'fit_denses.5.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.0.weight', 'cls.seq_relationship.weight', 'fit_denses.3.bias', 'fit_denses.1.bias', 'fit_denses.2.bias', 'fit_denses.0.bias', 'cls.predictions.bias', 'fit_denses.6.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.4.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.6.weight', 'fit_denses.4.bias', 'fit_denses.6.bias', 'fit_denses.3.weight', 'fit_denses.1.bias', 'fit_denses.0.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.5.weight', 'fit_denses.1.weight', 'fit_denses.2.weight', 'fit_denses.5.bias', 'fit_denses.3.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.0.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.6.weight', 'cls.seq_relationship.bias', 'fit_denses.1.weight', 'fit_denses.3.weight', 'fit_denses.5.weight', 'fit_denses.4.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.0.bias', 'fit_denses.2.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.2.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'fit_denses.5.bias', 'cls.predictions.bias', 'fit_denses.6.bias', 'fit_denses.3.bias', 'fit_denses.4.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.0.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.3.bias', 'fit_denses.6.weight', 'fit_denses.6.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.1.weight', 'fit_denses.5.bias', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.2.weight', 'fit_denses.5.weight', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.4.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'fit_denses.4.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.bias', 'fit_denses.3.weight', 'fit_denses.0.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.4.weight', 'fit_denses.3.weight', 'fit_denses.2.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.5.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.0.weight', 'fit_denses.6.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.1.weight', 'fit_denses.1.bias', 'fit_denses.0.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.2.weight', 'fit_denses.5.bias', 'fit_denses.4.bias', 'fit_denses.6.weight', 'fit_denses.3.bias', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.2.weight', 'fit_denses.3.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.4.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'fit_denses.4.bias', 'fit_denses.0.weight', 'fit_denses.5.weight', 'fit_denses.0.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.6.weight', 'cls.predictions.bias', 'fit_denses.1.weight', 'fit_denses.5.bias', 'fit_denses.1.bias', 'cls.seq_relationship.bias', 'fit_denses.6.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.6.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.2.weight', 'cls.predictions.decoder.weight', 'fit_denses.1.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.1.weight', 'fit_denses.2.bias', 'fit_denses.4.bias', 'fit_denses.5.bias', 'fit_denses.6.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.5.weight', 'fit_denses.3.bias', 'fit_denses.0.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'fit_denses.3.weight', 'cls.seq_relationship.bias', 'fit_denses.4.weight', 'fit_denses.0.bias', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'fit_denses.2.bias', 'fit_denses.1.bias', 'fit_denses.6.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.weight', 'fit_denses.5.bias', 'fit_denses.3.bias', 'fit_denses.5.weight', 'fit_denses.0.weight', 'cls.seq_relationship.weight', 'fit_denses.0.bias', 'cls.predictions.decoder.weight', 'fit_denses.4.bias', 'fit_denses.2.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'fit_denses.1.weight', 'fit_denses.6.bias', 'fit_denses.4.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'fit_denses.5.bias', 'fit_denses.0.bias', 'fit_denses.5.weight', 'fit_denses.2.weight', 'fit_denses.4.bias', 'fit_denses.3.bias', 'fit_denses.0.weight', 'fit_denses.1.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.4.weight', 'fit_denses.2.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'fit_denses.3.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.6.weight', 'cls.seq_relationship.bias', 'fit_denses.6.bias', 'fit_denses.1.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'fit_denses.4.bias', 'fit_denses.4.weight', 'fit_denses.5.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.5.bias', 'fit_denses.0.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.3.weight', 'fit_denses.1.weight', 'fit_denses.3.bias', 'fit_denses.2.bias', 'fit_denses.6.bias', 'fit_denses.1.bias', 'fit_denses.6.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.6.weight', 'cls.seq_relationship.weight', 'fit_denses.3.bias', 'cls.predictions.decoder.weight', 'fit_denses.1.bias', 'fit_denses.5.bias', 'fit_denses.4.weight', 'fit_denses.2.bias', 'cls.predictions.bias', 'fit_denses.3.weight', 'fit_denses.4.bias', 'cls.seq_relationship.bias', 'fit_denses.2.weight', 'fit_denses.0.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.1.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.0.weight', 'fit_denses.5.weight', 'fit_denses.6.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'fit_denses.2.weight', 'fit_denses.3.weight', 'fit_denses.3.bias', 'fit_denses.4.weight', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'fit_denses.5.weight', 'fit_denses.4.bias', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'fit_denses.1.weight', 'fit_denses.6.bias', 'fit_denses.5.bias', 'fit_denses.1.bias', 'fit_denses.0.weight', 'fit_denses.6.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.5.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.0.bias', 'fit_denses.5.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.weight', 'fit_denses.4.weight', 'cls.seq_relationship.bias', 'fit_denses.6.bias', 'fit_denses.2.weight', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'fit_denses.3.weight', 'fit_denses.4.bias', 'fit_denses.0.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.2.bias', 'fit_denses.6.weight', 'fit_denses.3.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'fit_denses.2.weight', 'cls.predictions.decoder.weight', 'fit_denses.3.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'fit_denses.4.weight', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.1.weight', 'fit_denses.6.bias', 'fit_denses.5.weight', 'fit_denses.3.bias', 'fit_denses.0.weight', 'fit_denses.5.bias', 'fit_denses.1.bias', 'fit_denses.0.bias', 'fit_denses.4.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.6.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.0.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'fit_denses.1.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.1.bias', 'fit_denses.4.weight', 'fit_denses.6.bias', 'fit_denses.5.bias', 'fit_denses.3.weight', 'fit_denses.4.bias', 'cls.predictions.decoder.weight', 'fit_denses.5.weight', 'cls.seq_relationship.bias', 'fit_denses.2.bias', 'fit_denses.0.weight', 'fit_denses.2.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.6.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'fit_denses.0.weight', 'cls.predictions.bias', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.5.weight', 'fit_denses.3.bias', 'fit_denses.6.weight', 'fit_denses.3.weight', 'fit_denses.1.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'fit_denses.1.bias', 'fit_denses.6.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.4.weight', 'fit_denses.5.bias', 'fit_denses.0.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.2.weight', 'fit_denses.4.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'fit_denses.6.weight', 'cls.seq_relationship.bias', 'fit_denses.0.bias', 'fit_denses.1.weight', 'cls.predictions.bias', 'fit_denses.2.bias', 'fit_denses.4.weight', 'fit_denses.1.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.0.weight', 'fit_denses.4.bias', 'fit_denses.3.weight', 'fit_denses.6.bias', 'fit_denses.3.bias', 'cls.predictions.decoder.weight', 'fit_denses.5.weight', 'fit_denses.5.bias', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.2.weight', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of the model checkpoint at huawei-noah/TinyBERT_General_6L_768D were not used when initializing BertForSequenceClassification: ['fit_denses.2.weight', 'fit_denses.1.bias', 'fit_denses.0.weight', 'fit_denses.6.weight', 'cls.predictions.transform.LayerNorm.weight', 'fit_denses.3.weight', 'fit_denses.1.weight', 'cls.seq_relationship.weight', 'fit_denses.4.weight', 'fit_denses.5.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.0.bias', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'fit_denses.6.bias', 'cls.predictions.decoder.weight', 'fit_denses.3.bias', 'fit_denses.2.bias', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.5.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_6L_768D and are newly initialized: ['classifier.weight', 'classifier.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 23.74it/s]100%|██████████| 3/3 [00:00<00:00, 23.70it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
INFO flwr 2023-12-18 02:06:51,447 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flwr 2023-12-18 02:06:51,694 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:06:51,694 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:06:52,248 | connection.py:42 | ChannelConnectivity.READY
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 292.02it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 375.47it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 712.99it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 202.15it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 670.55it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 405.16it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 170.47it/s]
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 668.73it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 676.72it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
  0%|          | 0/3 [00:00<?, ?it/s]Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
100%|██████████| 3/3 [00:00<00:00, 211.45it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 535.90it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 595.61it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 230.30it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 235.33it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 222.54it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
INFO flwr 2023-12-18 02:07:04,300 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 703.47it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
INFO flwr 2023-12-18 02:07:04,571 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 252.64it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
INFO flwr 2023-12-18 02:07:04,768 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
INFO flwr 2023-12-18 02:07:04,803 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 638.86it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
INFO flwr 2023-12-18 02:07:04,826 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 489.66it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
  0%|          | 0/3 [00:00<?, ?it/s]INFO flwr 2023-12-18 02:07:04,915 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
100%|██████████| 3/3 [00:00<00:00, 374.68it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
INFO flwr 2023-12-18 02:07:04,972 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 269.62it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
DEBUG flwr 2023-12-18 02:07:05,037 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:05,038 | connection.py:42 | ChannelConnectivity.CONNECTING
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
  0%|          | 0/3 [00:00<?, ?it/s]Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
100%|██████████| 3/3 [00:00<00:00, 212.06it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
DEBUG flwr 2023-12-18 02:07:05,451 | connection.py:42 | ChannelConnectivity.READY
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
INFO flwr 2023-12-18 02:07:05,341 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:05,529 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
Reusing dataset glue (/scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)
INFO flwr 2023-12-18 02:07:05,635 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:05,729 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:05,752 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:05,805 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:05,870 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
  0%|          | 0/3 [00:00<?, ?it/s]100%|██████████| 3/3 [00:00<00:00, 225.20it/s]
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-93aff4f4dcaeb739.arrow
INFO flwr 2023-12-18 02:07:05,966 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-d2ef9dcd59b2bf3f.arrow
DEBUG flwr 2023-12-18 02:07:05,998 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,293 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:05,999 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,294 | connection.py:42 | ChannelConnectivity.READY
INFO flwr 2023-12-18 02:07:06,083 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:06,163 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
INFO flwr 2023-12-18 02:07:06,184 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
Loading cached processed dataset at /scratch/ml9027/.cache/glue/cola/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-23629d86c01ce6fb.arrow
INFO flwr 2023-12-18 02:07:06,416 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flwr 2023-12-18 02:07:06,428 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,428 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,446 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,476 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,498 | connection.py:42 | ChannelConnectivity.IDLE
INFO flwr 2023-12-18 02:07:06,512 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flwr 2023-12-18 02:07:06,512 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,514 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,549 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,560 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,562 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,567 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,567 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,568 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,586 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,586 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:06,587 | connection.py:42 | ChannelConnectivity.CONNECTING
INFO flwr 2023-12-18 02:07:06,826 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flwr 2023-12-18 02:07:06,884 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,885 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:06,898 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,898 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,899 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,900 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,901 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,901 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,901 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,901 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:06,989 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:07,010 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,010 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:07,011 | connection.py:42 | ChannelConnectivity.READY
INFO flwr 2023-12-18 02:07:07,044 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flwr 2023-12-18 02:07:07,099 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,099 | connection.py:42 | ChannelConnectivity.CONNECTING
INFO flwr 2023-12-18 02:07:07,171 | grpc.py:52 | Opened insecure gRPC connection (no certificates were passed)
DEBUG flwr 2023-12-18 02:07:07,451 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,494 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:07,515 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,520 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:07,523 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,539 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:07,542 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,544 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:07,696 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,698 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:07,878 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:07,897 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:08,183 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:08,212 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:09,075 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:09,075 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:09,075 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:09,076 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:09,076 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:09,076 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:09,149 | connection.py:42 | ChannelConnectivity.IDLE
DEBUG flwr 2023-12-18 02:07:09,149 | connection.py:42 | ChannelConnectivity.CONNECTING
DEBUG flwr 2023-12-18 02:07:09,913 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:10,016 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:10,049 | connection.py:42 | ChannelConnectivity.READY
DEBUG flwr 2023-12-18 02:07:10,131 | connection.py:42 | ChannelConnectivity.READY
INFO flwr 2023-12-18 02:07:12,812 | server.py:280 | Received initial parameters from one random client
INFO flwr 2023-12-18 02:07:12,812 | server.py:91 | Evaluating initial parameters
INFO flwr 2023-12-18 02:07:12,812 | server.py:104 | FL starting
DEBUG flwr 2023-12-18 02:07:12,812 | server.py:222 | fit_round 1: strategy sampled 24 clients (out of 24)
DEBUG flwr 2023-12-18 02:08:35,159 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
DEBUG flwr 2023-12-18 02:08:35,218 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
DEBUG flwr 2023-12-18 02:08:35,227 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
DEBUG flwr 2023-12-18 02:08:35,228 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
DEBUG flwr 2023-12-18 02:08:35,229 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
DEBUG flwr 2023-12-18 02:08:35,231 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
DEBUG flwr 2023-12-18 02:08:35,247 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
DEBUG flwr 2023-12-18 02:08:35,253 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
DEBUG flwr 2023-12-18 02:08:35,265 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
DEBUG flwr 2023-12-18 02:08:35,270 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
DEBUG flwr 2023-12-18 02:08:35,286 | connection.py:141 | gRPC channel closed
Training 1 epoch(s) w/ 8551 batches each
Traceback (most recent call last):
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 73, in <module>
    main()
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 69, in main
    fl.client.start_numpy_client(server_address="0.0.0.0:8080", client=client)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 401, in start_numpy_client
    start_client(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/app.py", line 294, in start_client
    bwd_msg: Bwd = app(fwd=fwd_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/flower.py", line 82, in __call__
    task_res = handle(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 117, in handle
    client_msg = handle_legacy_message(client_fn, server_msg)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 153, in handle_legacy_message
    return _fit(client, server_msg.fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/message_handler/message_handler.py", line 212, in _fit
    fit_res = maybe_call_fit(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/client.py", line 223, in maybe_call_fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    return client.fit(fit_ins)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/flwr/client/numpy_client.py", line 227, in _fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    results = self.numpy_client.fit(parameters, ins.config)  # type: ignore
  File "/scratch/ml9027/ming/lamp-main/flwr_client.py", line 46, in fit
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 116, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    secagg.trainOne(self.model, self.trainloader, epochs=1, device=DEVICE)
  File "/scratch/ml9027/ming/lamp-main/secagg.py", line 106, in trainOne
    optimizer.step()
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/optim/optimizer.py", line 88, in wrapper
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = net(**data)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return func(*args, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/autograd/grad_mode.py", line 28, in decorate_context
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 1540, in forward
    return func(*args, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/optim/sgd.py", line 136, in step
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    F.sgd(params_with_grad,
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/optim/_functional.py", line 170, in sgd
    outputs = self.bert(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 999, in forward
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    encoder_outputs = self.encoder(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 585, in forward
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    buf = torch.clone(d_p).detach()
RuntimeError: CUDA out of memory. Tried to allocate 90.00 MiB (GPU 0; 44.48 GiB total capacity; 513.01 MiB already allocated; 41.31 MiB free; 566.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    layer_outputs = layer_module(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 472, in forward
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_attention_outputs = self.attention(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 402, in forward
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    self_outputs = self.self(
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/transformers/models/bert/modeling_bert.py", line 268, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    mixed_query_layer = self.query(hidden_states)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return forward_call(*input, **kwargs)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 103, in forward
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return F.linear(input, self.weight, self.bias)
  File "/scratch/ml9027/lamp-main/penv/lib/python3.9/site-packages/torch/nn/functional.py", line 1848, in linear
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
    return torch._C._nn.linear(input, weight, bias)
RuntimeError: CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `cublasCreate(handle)`
DEBUG flwr 2023-12-18 02:08:44,718 | server.py:236 | fit_round 1 received 13 results and 11 failures
WARNING flwr 2023-12-18 02:08:51,278 | fedavg.py:242 | No fit_metrics_aggregation_fn provided
DEBUG flwr 2023-12-18 02:08:51,285 | server.py:173 | evaluate_round 1: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:11:01,345 | server.py:187 | evaluate_round 1 received 13 results and 0 failures
WARNING flwr 2023-12-18 02:11:01,388 | fedavg.py:273 | No evaluate_metrics_aggregation_fn provided
DEBUG flwr 2023-12-18 02:11:01,388 | server.py:222 | fit_round 2: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:12:01,615 | server.py:236 | fit_round 2 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:12:07,027 | server.py:173 | evaluate_round 2: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:14:11,854 | server.py:187 | evaluate_round 2 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:14:11,854 | server.py:222 | fit_round 3: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:15:11,296 | server.py:236 | fit_round 3 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:15:15,657 | server.py:173 | evaluate_round 3: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:17:18,704 | server.py:187 | evaluate_round 3 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:17:18,704 | server.py:222 | fit_round 4: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:18:18,424 | server.py:236 | fit_round 4 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:18:22,500 | server.py:173 | evaluate_round 4: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:20:27,145 | server.py:187 | evaluate_round 4 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:20:27,145 | server.py:222 | fit_round 5: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:21:27,821 | server.py:236 | fit_round 5 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:21:31,440 | server.py:173 | evaluate_round 5: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:23:36,464 | server.py:187 | evaluate_round 5 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:23:36,465 | server.py:222 | fit_round 6: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:24:37,094 | server.py:236 | fit_round 6 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:24:41,536 | server.py:173 | evaluate_round 6: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:26:45,884 | server.py:187 | evaluate_round 6 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:26:45,896 | server.py:222 | fit_round 7: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:27:44,440 | server.py:236 | fit_round 7 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:27:47,863 | server.py:173 | evaluate_round 7: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:29:51,753 | server.py:187 | evaluate_round 7 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:29:51,754 | server.py:222 | fit_round 8: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:30:49,720 | server.py:236 | fit_round 8 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:30:53,912 | server.py:173 | evaluate_round 8: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:32:57,303 | server.py:187 | evaluate_round 8 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:32:57,304 | server.py:222 | fit_round 9: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:33:55,054 | server.py:236 | fit_round 9 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:33:58,597 | server.py:173 | evaluate_round 9: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:36:00,695 | server.py:187 | evaluate_round 9 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:36:00,695 | server.py:222 | fit_round 10: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:37:00,915 | server.py:236 | fit_round 10 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:37:04,869 | server.py:173 | evaluate_round 10: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:39:08,955 | server.py:187 | evaluate_round 10 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:39:08,955 | server.py:222 | fit_round 11: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:40:07,616 | server.py:236 | fit_round 11 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:40:12,140 | server.py:173 | evaluate_round 11: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:42:16,436 | server.py:187 | evaluate_round 11 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:42:16,437 | server.py:222 | fit_round 12: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:43:16,223 | server.py:236 | fit_round 12 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:43:19,801 | server.py:173 | evaluate_round 12: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:45:23,460 | server.py:187 | evaluate_round 12 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:45:23,461 | server.py:222 | fit_round 13: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:46:21,263 | server.py:236 | fit_round 13 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:46:25,699 | server.py:173 | evaluate_round 13: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:48:28,264 | server.py:187 | evaluate_round 13 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:48:28,264 | server.py:222 | fit_round 14: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:49:26,508 | server.py:236 | fit_round 14 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:49:30,561 | server.py:173 | evaluate_round 14: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:51:34,574 | server.py:187 | evaluate_round 14 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:51:34,574 | server.py:222 | fit_round 15: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:52:32,551 | server.py:236 | fit_round 15 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:52:36,865 | server.py:173 | evaluate_round 15: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:54:40,239 | server.py:187 | evaluate_round 15 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:54:40,239 | server.py:222 | fit_round 16: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:55:38,689 | server.py:236 | fit_round 16 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:55:42,101 | server.py:173 | evaluate_round 16: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:57:45,833 | server.py:187 | evaluate_round 16 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:57:45,833 | server.py:222 | fit_round 17: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 02:58:45,494 | server.py:236 | fit_round 17 received 13 results and 0 failures
DEBUG flwr 2023-12-18 02:58:49,331 | server.py:173 | evaluate_round 17: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:00:53,376 | server.py:187 | evaluate_round 17 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:00:53,377 | server.py:222 | fit_round 18: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:01:51,816 | server.py:236 | fit_round 18 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:01:56,126 | server.py:173 | evaluate_round 18: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:04:00,568 | server.py:187 | evaluate_round 18 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:04:00,568 | server.py:222 | fit_round 19: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:04:58,991 | server.py:236 | fit_round 19 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:05:02,760 | server.py:173 | evaluate_round 19: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:07:05,854 | server.py:187 | evaluate_round 19 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:07:05,854 | server.py:222 | fit_round 20: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:08:04,178 | server.py:236 | fit_round 20 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:08:08,531 | server.py:173 | evaluate_round 20: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:10:11,910 | server.py:187 | evaluate_round 20 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:10:11,910 | server.py:222 | fit_round 21: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:11:10,094 | server.py:236 | fit_round 21 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:11:13,810 | server.py:173 | evaluate_round 21: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:13:18,052 | server.py:187 | evaluate_round 21 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:13:18,052 | server.py:222 | fit_round 22: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:14:16,589 | server.py:236 | fit_round 22 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:14:20,597 | server.py:173 | evaluate_round 22: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:16:22,866 | server.py:187 | evaluate_round 22 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:16:22,866 | server.py:222 | fit_round 23: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:17:21,418 | server.py:236 | fit_round 23 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:17:25,404 | server.py:173 | evaluate_round 23: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:19:29,221 | server.py:187 | evaluate_round 23 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:19:29,221 | server.py:222 | fit_round 24: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:20:27,601 | server.py:236 | fit_round 24 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:20:31,297 | server.py:173 | evaluate_round 24: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:22:35,639 | server.py:187 | evaluate_round 24 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:22:35,640 | server.py:222 | fit_round 25: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:23:33,932 | server.py:236 | fit_round 25 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:23:38,399 | server.py:173 | evaluate_round 25: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:25:42,683 | server.py:187 | evaluate_round 25 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:25:42,702 | server.py:222 | fit_round 26: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:26:41,822 | server.py:236 | fit_round 26 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:26:45,163 | server.py:173 | evaluate_round 26: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:28:48,337 | server.py:187 | evaluate_round 26 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:28:48,338 | server.py:222 | fit_round 27: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:29:44,885 | server.py:236 | fit_round 27 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:29:49,675 | server.py:173 | evaluate_round 27: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:31:52,947 | server.py:187 | evaluate_round 27 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:31:52,947 | server.py:222 | fit_round 28: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:32:51,742 | server.py:236 | fit_round 28 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:32:55,101 | server.py:173 | evaluate_round 28: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:34:59,159 | server.py:187 | evaluate_round 28 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:34:59,159 | server.py:222 | fit_round 29: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:35:57,616 | server.py:236 | fit_round 29 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:36:01,654 | server.py:173 | evaluate_round 29: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:38:05,505 | server.py:187 | evaluate_round 29 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:38:05,505 | server.py:222 | fit_round 30: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:39:03,506 | server.py:236 | fit_round 30 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:39:07,413 | server.py:173 | evaluate_round 30: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:41:11,849 | server.py:187 | evaluate_round 30 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:41:11,849 | server.py:222 | fit_round 31: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:42:11,142 | server.py:236 | fit_round 31 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:42:14,901 | server.py:173 | evaluate_round 31: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:44:18,677 | server.py:187 | evaluate_round 31 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:44:18,677 | server.py:222 | fit_round 32: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:45:17,541 | server.py:236 | fit_round 32 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:45:21,562 | server.py:173 | evaluate_round 32: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:47:24,869 | server.py:187 | evaluate_round 32 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:47:24,869 | server.py:222 | fit_round 33: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:48:23,418 | server.py:236 | fit_round 33 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:48:27,155 | server.py:173 | evaluate_round 33: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:50:31,272 | server.py:187 | evaluate_round 33 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:50:31,272 | server.py:222 | fit_round 34: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:51:29,391 | server.py:236 | fit_round 34 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:51:33,741 | server.py:173 | evaluate_round 34: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:53:37,631 | server.py:187 | evaluate_round 34 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:53:37,631 | server.py:222 | fit_round 35: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:54:35,053 | server.py:236 | fit_round 35 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:54:38,495 | server.py:173 | evaluate_round 35: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:56:41,376 | server.py:187 | evaluate_round 35 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:56:41,392 | server.py:222 | fit_round 36: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:57:40,757 | server.py:236 | fit_round 36 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:57:44,921 | server.py:173 | evaluate_round 36: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 03:59:48,528 | server.py:187 | evaluate_round 36 received 13 results and 0 failures
DEBUG flwr 2023-12-18 03:59:48,529 | server.py:222 | fit_round 37: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:00:46,049 | server.py:236 | fit_round 37 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:00:49,511 | server.py:173 | evaluate_round 37: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:02:53,800 | server.py:187 | evaluate_round 37 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:02:53,800 | server.py:222 | fit_round 38: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:03:52,990 | server.py:236 | fit_round 38 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:03:56,720 | server.py:173 | evaluate_round 38: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:05:59,940 | server.py:187 | evaluate_round 38 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:05:59,941 | server.py:222 | fit_round 39: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:06:57,682 | server.py:236 | fit_round 39 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:07:01,628 | server.py:173 | evaluate_round 39: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:09:04,309 | server.py:187 | evaluate_round 39 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:09:04,309 | server.py:222 | fit_round 40: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:10:03,046 | server.py:236 | fit_round 40 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:10:06,489 | server.py:173 | evaluate_round 40: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:12:09,378 | server.py:187 | evaluate_round 40 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:12:09,379 | server.py:222 | fit_round 41: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:13:06,665 | server.py:236 | fit_round 41 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:13:10,972 | server.py:173 | evaluate_round 41: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:15:14,706 | server.py:187 | evaluate_round 41 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:15:14,707 | server.py:222 | fit_round 42: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:16:13,160 | server.py:236 | fit_round 42 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:16:16,396 | server.py:173 | evaluate_round 42: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:18:19,426 | server.py:187 | evaluate_round 42 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:18:19,426 | server.py:222 | fit_round 43: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:19:17,747 | server.py:236 | fit_round 43 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:19:22,316 | server.py:173 | evaluate_round 43: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:21:25,339 | server.py:187 | evaluate_round 43 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:21:25,340 | server.py:222 | fit_round 44: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:22:24,222 | server.py:236 | fit_round 44 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:22:27,793 | server.py:173 | evaluate_round 44: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:24:31,393 | server.py:187 | evaluate_round 44 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:24:31,394 | server.py:222 | fit_round 45: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:25:30,707 | server.py:236 | fit_round 45 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:25:34,494 | server.py:173 | evaluate_round 45: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:27:37,397 | server.py:187 | evaluate_round 45 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:27:37,412 | server.py:222 | fit_round 46: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:28:36,362 | server.py:236 | fit_round 46 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:28:40,390 | server.py:173 | evaluate_round 46: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:30:43,851 | server.py:187 | evaluate_round 46 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:30:43,851 | server.py:222 | fit_round 47: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:31:43,035 | server.py:236 | fit_round 47 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:31:46,407 | server.py:173 | evaluate_round 47: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:33:49,404 | server.py:187 | evaluate_round 47 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:33:49,404 | server.py:222 | fit_round 48: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:34:47,948 | server.py:236 | fit_round 48 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:34:52,085 | server.py:173 | evaluate_round 48: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:36:54,939 | server.py:187 | evaluate_round 48 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:36:54,939 | server.py:222 | fit_round 49: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:37:53,402 | server.py:236 | fit_round 49 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:37:57,247 | server.py:173 | evaluate_round 49: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:40:00,670 | server.py:187 | evaluate_round 49 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:40:00,670 | server.py:222 | fit_round 50: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:40:58,603 | server.py:236 | fit_round 50 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:41:02,452 | server.py:173 | evaluate_round 50: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:43:11,334 | server.py:187 | evaluate_round 50 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:43:11,335 | server.py:222 | fit_round 51: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:44:14,815 | server.py:236 | fit_round 51 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:44:19,572 | server.py:173 | evaluate_round 51: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:46:31,862 | server.py:187 | evaluate_round 51 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:46:31,863 | server.py:222 | fit_round 52: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:47:34,347 | server.py:236 | fit_round 52 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:47:38,011 | server.py:173 | evaluate_round 52: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:49:41,788 | server.py:187 | evaluate_round 52 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:49:41,789 | server.py:222 | fit_round 53: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:50:40,315 | server.py:236 | fit_round 53 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:50:44,425 | server.py:173 | evaluate_round 53: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:52:47,782 | server.py:187 | evaluate_round 53 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:52:47,782 | server.py:222 | fit_round 54: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:53:46,832 | server.py:236 | fit_round 54 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:53:50,223 | server.py:173 | evaluate_round 54: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:55:53,653 | server.py:187 | evaluate_round 54 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:55:53,671 | server.py:222 | fit_round 55: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:56:51,579 | server.py:236 | fit_round 55 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:56:55,572 | server.py:173 | evaluate_round 55: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:58:59,125 | server.py:187 | evaluate_round 55 received 13 results and 0 failures
DEBUG flwr 2023-12-18 04:58:59,126 | server.py:222 | fit_round 56: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 04:59:56,836 | server.py:236 | fit_round 56 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:00:00,451 | server.py:173 | evaluate_round 56: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:02:04,041 | server.py:187 | evaluate_round 56 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:02:04,041 | server.py:222 | fit_round 57: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:03:02,416 | server.py:236 | fit_round 57 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:03:06,462 | server.py:173 | evaluate_round 57: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:05:10,007 | server.py:187 | evaluate_round 57 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:05:10,007 | server.py:222 | fit_round 58: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:06:07,814 | server.py:236 | fit_round 58 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:06:11,751 | server.py:173 | evaluate_round 58: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:08:15,524 | server.py:187 | evaluate_round 58 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:08:15,524 | server.py:222 | fit_round 59: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:09:14,041 | server.py:236 | fit_round 59 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:09:18,027 | server.py:173 | evaluate_round 59: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:11:20,271 | server.py:187 | evaluate_round 59 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:11:20,271 | server.py:222 | fit_round 60: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:12:17,357 | server.py:236 | fit_round 60 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:12:21,574 | server.py:173 | evaluate_round 60: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:14:25,704 | server.py:187 | evaluate_round 60 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:14:25,704 | server.py:222 | fit_round 61: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:15:25,164 | server.py:236 | fit_round 61 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:15:28,469 | server.py:173 | evaluate_round 61: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:17:31,203 | server.py:187 | evaluate_round 61 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:17:31,204 | server.py:222 | fit_round 62: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:18:30,682 | server.py:236 | fit_round 62 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:18:35,395 | server.py:173 | evaluate_round 62: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:20:39,228 | server.py:187 | evaluate_round 62 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:20:39,228 | server.py:222 | fit_round 63: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:21:38,378 | server.py:236 | fit_round 63 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:21:41,923 | server.py:173 | evaluate_round 63: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:23:45,184 | server.py:187 | evaluate_round 63 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:23:45,185 | server.py:222 | fit_round 64: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:24:44,591 | server.py:236 | fit_round 64 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:24:48,705 | server.py:173 | evaluate_round 64: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:26:51,560 | server.py:187 | evaluate_round 64 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:26:51,577 | server.py:222 | fit_round 65: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:27:49,124 | server.py:236 | fit_round 65 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:27:53,135 | server.py:173 | evaluate_round 65: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:29:58,936 | server.py:187 | evaluate_round 65 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:29:58,937 | server.py:222 | fit_round 66: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:30:59,266 | server.py:236 | fit_round 66 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:31:03,008 | server.py:173 | evaluate_round 66: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:33:08,609 | server.py:187 | evaluate_round 66 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:33:08,610 | server.py:222 | fit_round 67: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:34:08,284 | server.py:236 | fit_round 67 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:34:13,065 | server.py:173 | evaluate_round 67: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:36:16,300 | server.py:187 | evaluate_round 67 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:36:16,300 | server.py:222 | fit_round 68: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:37:15,017 | server.py:236 | fit_round 68 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:37:18,493 | server.py:173 | evaluate_round 68: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:39:24,533 | server.py:187 | evaluate_round 68 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:39:24,541 | server.py:222 | fit_round 69: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:40:24,511 | server.py:236 | fit_round 69 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:40:28,916 | server.py:173 | evaluate_round 69: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:42:35,127 | server.py:187 | evaluate_round 69 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:42:35,127 | server.py:222 | fit_round 70: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:43:34,612 | server.py:236 | fit_round 70 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:43:38,827 | server.py:173 | evaluate_round 70: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:45:45,461 | server.py:187 | evaluate_round 70 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:45:45,461 | server.py:222 | fit_round 71: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:46:45,275 | server.py:236 | fit_round 71 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:46:49,543 | server.py:173 | evaluate_round 71: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:48:57,196 | server.py:187 | evaluate_round 71 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:48:57,196 | server.py:222 | fit_round 72: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:49:56,948 | server.py:236 | fit_round 72 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:50:01,116 | server.py:173 | evaluate_round 72: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:52:08,085 | server.py:187 | evaluate_round 72 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:52:08,085 | server.py:222 | fit_round 73: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:53:07,993 | server.py:236 | fit_round 73 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:53:11,938 | server.py:173 | evaluate_round 73: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:55:18,477 | server.py:187 | evaluate_round 73 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:55:18,494 | server.py:222 | fit_round 74: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:56:16,363 | server.py:236 | fit_round 74 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:56:20,463 | server.py:173 | evaluate_round 74: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:58:25,706 | server.py:187 | evaluate_round 74 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:58:25,719 | server.py:222 | fit_round 75: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 05:59:25,955 | server.py:236 | fit_round 75 received 13 results and 0 failures
DEBUG flwr 2023-12-18 05:59:29,414 | server.py:173 | evaluate_round 75: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:01:37,047 | server.py:187 | evaluate_round 75 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:01:37,047 | server.py:222 | fit_round 76: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:02:37,798 | server.py:236 | fit_round 76 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:02:42,035 | server.py:173 | evaluate_round 76: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:04:49,023 | server.py:187 | evaluate_round 76 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:04:49,023 | server.py:222 | fit_round 77: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:05:49,173 | server.py:236 | fit_round 77 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:05:52,734 | server.py:173 | evaluate_round 77: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:08:00,232 | server.py:187 | evaluate_round 77 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:08:00,232 | server.py:222 | fit_round 78: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:09:01,117 | server.py:236 | fit_round 78 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:09:05,123 | server.py:173 | evaluate_round 78: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:11:12,977 | server.py:187 | evaluate_round 78 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:11:12,977 | server.py:222 | fit_round 79: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:12:12,321 | server.py:236 | fit_round 79 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:12:16,094 | server.py:173 | evaluate_round 79: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:14:23,456 | server.py:187 | evaluate_round 79 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:14:23,463 | server.py:222 | fit_round 80: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:15:25,186 | server.py:236 | fit_round 80 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:15:28,750 | server.py:173 | evaluate_round 80: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:17:36,625 | server.py:187 | evaluate_round 80 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:17:36,626 | server.py:222 | fit_round 81: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:18:37,270 | server.py:236 | fit_round 81 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:18:41,995 | server.py:173 | evaluate_round 81: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:20:48,924 | server.py:187 | evaluate_round 81 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:20:48,924 | server.py:222 | fit_round 82: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:21:50,129 | server.py:236 | fit_round 82 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:21:53,434 | server.py:173 | evaluate_round 82: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:24:01,442 | server.py:187 | evaluate_round 82 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:24:01,442 | server.py:222 | fit_round 83: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:25:01,201 | server.py:236 | fit_round 83 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:25:05,318 | server.py:173 | evaluate_round 83: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:27:13,179 | server.py:187 | evaluate_round 83 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:27:13,196 | server.py:222 | fit_round 84: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:28:12,928 | server.py:236 | fit_round 84 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:28:16,356 | server.py:173 | evaluate_round 84: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:30:23,819 | server.py:187 | evaluate_round 84 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:30:23,819 | server.py:222 | fit_round 85: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:31:23,230 | server.py:236 | fit_round 85 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:31:27,932 | server.py:173 | evaluate_round 85: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:33:35,688 | server.py:187 | evaluate_round 85 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:33:35,688 | server.py:222 | fit_round 86: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:34:35,597 | server.py:236 | fit_round 86 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:34:39,042 | server.py:173 | evaluate_round 86: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:36:45,229 | server.py:187 | evaluate_round 86 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:36:45,229 | server.py:222 | fit_round 87: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:37:45,531 | server.py:236 | fit_round 87 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:37:49,249 | server.py:173 | evaluate_round 87: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:39:57,004 | server.py:187 | evaluate_round 87 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:39:57,010 | server.py:222 | fit_round 88: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:40:56,361 | server.py:236 | fit_round 88 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:41:00,827 | server.py:173 | evaluate_round 88: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:43:08,170 | server.py:187 | evaluate_round 88 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:43:08,171 | server.py:222 | fit_round 89: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:44:08,779 | server.py:236 | fit_round 89 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:44:12,172 | server.py:173 | evaluate_round 89: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:46:20,234 | server.py:187 | evaluate_round 89 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:46:20,234 | server.py:222 | fit_round 90: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:47:20,030 | server.py:236 | fit_round 90 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:47:24,307 | server.py:173 | evaluate_round 90: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:49:30,843 | server.py:187 | evaluate_round 90 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:49:30,843 | server.py:222 | fit_round 91: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:50:30,855 | server.py:236 | fit_round 91 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:50:34,161 | server.py:173 | evaluate_round 91: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:52:41,089 | server.py:187 | evaluate_round 91 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:52:41,089 | server.py:222 | fit_round 92: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:53:40,422 | server.py:236 | fit_round 92 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:53:44,550 | server.py:173 | evaluate_round 92: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:55:49,485 | server.py:187 | evaluate_round 92 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:55:49,486 | server.py:222 | fit_round 93: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:56:48,144 | server.py:236 | fit_round 93 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:56:51,398 | server.py:173 | evaluate_round 93: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:58:57,650 | server.py:187 | evaluate_round 93 received 13 results and 0 failures
DEBUG flwr 2023-12-18 06:58:57,650 | server.py:222 | fit_round 94: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 06:59:56,653 | server.py:236 | fit_round 94 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:00:00,580 | server.py:173 | evaluate_round 94: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:02:05,604 | server.py:187 | evaluate_round 94 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:02:05,604 | server.py:222 | fit_round 95: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:03:04,150 | server.py:236 | fit_round 95 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:03:07,582 | server.py:173 | evaluate_round 95: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:05:12,523 | server.py:187 | evaluate_round 95 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:05:12,523 | server.py:222 | fit_round 96: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:06:12,339 | server.py:236 | fit_round 96 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:06:15,675 | server.py:173 | evaluate_round 96: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:08:20,861 | server.py:187 | evaluate_round 96 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:08:20,861 | server.py:222 | fit_round 97: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:09:18,887 | server.py:236 | fit_round 97 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:09:22,758 | server.py:173 | evaluate_round 97: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:11:25,930 | server.py:187 | evaluate_round 97 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:11:25,930 | server.py:222 | fit_round 98: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:12:24,344 | server.py:236 | fit_round 98 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:12:27,386 | server.py:173 | evaluate_round 98: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:14:31,155 | server.py:187 | evaluate_round 98 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:14:31,155 | server.py:222 | fit_round 99: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:15:28,210 | server.py:236 | fit_round 99 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:15:32,405 | server.py:173 | evaluate_round 99: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:17:35,353 | server.py:187 | evaluate_round 99 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:17:35,359 | server.py:222 | fit_round 100: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:18:33,445 | server.py:236 | fit_round 100 received 13 results and 0 failures
DEBUG flwr 2023-12-18 07:18:36,601 | server.py:173 | evaluate_round 100: strategy sampled 13 clients (out of 13)
DEBUG flwr 2023-12-18 07:20:40,347 | server.py:187 | evaluate_round 100 received 13 results and 0 failures
INFO flwr 2023-12-18 07:20:40,347 | server.py:153 | FL finished in 18807.535186616704
INFO flwr 2023-12-18 07:20:40,468 | app.py:226 | app_fit: losses_distributed [(1, 0.690837378685291), (2, 0.6937444989497845), (3, 0.6951846434519842), (4, 0.6919554884617145), (5, 0.6915916525400602), (6, 0.6906910813771762), (7, 0.6927288403877845), (8, 0.6857474583845872), (9, 0.6840258424098675), (10, 0.6836369633674622), (11, 0.6973550869868352), (12, 0.6832786798477173), (13, 0.6827796651766851), (14, 0.6846372255912194), (15, 0.6909871559876662), (16, 0.6779894232749939), (17, 0.6822995497630193), (18, 0.6682179478498605), (19, 0.6731124015954825), (20, 0.6910324784425589), (21, 0.6723967698904184), (22, 0.6734410249269925), (23, 0.6729330787291894), (24, 0.634787747493157), (25, 0.6809118114984952), (26, 0.6372919495289142), (27, 0.6618033234889691), (28, 0.6592942017775315), (29, 0.6322888548557575), (30, 0.6295880950414218), (31, 0.6410816953732417), (32, 0.6362280249595642), (33, 0.6910312634248), (34, 0.6568326858373789), (35, 0.6496340403190026), (36, 0.6729979331676776), (37, 0.6491407843736502), (38, 0.591110999767597), (39, 0.6465912048633282), (40, 0.6699558450625493), (41, 0.6214519005555373), (42, 0.6420068007249099), (43, 0.5710859986451956), (44, 0.7160730820435744), (45, 0.618454593878526), (46, 0.6712285922123835), (47, 0.6224755002902105), (48, 0.6443143211878263), (49, 0.6235024195451003), (50, 0.6438276125834539), (51, 0.6602599895917453), (52, 0.5578632538135235), (53, 0.6326698431601891), (54, 0.5817020673018235), (55, 0.6091327942334689), (56, 0.6057053345900315), (57, 0.6092823697970464), (58, 0.5718153073237493), (59, 0.5952139473878421), (60, 0.6349515983691583), (61, 0.5613764547384702), (62, 0.6967737583013681), (63, 0.6389561914480649), (64, 0.5290464690098395), (65, 0.5956683250573965), (66, 0.6269872326117295), (67, 0.5888130756524893), (68, 0.5910694576226748), (69, 0.5535739843661969), (70, 0.6340438173367426), (71, 0.6335026667668269), (72, 0.6969825487870437), (73, 0.5911179460011996), (74, 0.6295701998930711), (75, 0.5538899944378779), (76, 0.6267167971684382), (77, 0.6715297630200019), (78, 0.5460145702728858), (79, 0.6305842582996075), (80, 0.623597291799692), (81, 0.6649002753771268), (82, 0.5744296862528875), (83, 0.5812532580815829), (84, 0.6237664062243241), (85, 0.7512478759655585), (86, 0.5874188588215754), (87, 0.7467881395266607), (88, 0.543987920651069), (89, 0.538315685895773), (90, 0.6262550537402813), (91, 0.6121108646576221), (92, 0.6195881435504327), (93, 0.6652149718541366), (94, 0.6270244855147141), (95, 0.6211887231239905), (96, 0.535278476201571), (97, 0.6612063096119807), (98, 0.6665304509493021), (99, 0.6267389219540817), (100, 0.5330140796991495)]
INFO flwr 2023-12-18 07:20:40,488 | app.py:227 | app_fit: metrics_distributed_fit {}
INFO flwr 2023-12-18 07:20:40,488 | app.py:228 | app_fit: metrics_distributed {}
INFO flwr 2023-12-18 07:20:40,488 | app.py:229 | app_fit: losses_centralized []
INFO flwr 2023-12-18 07:20:40,488 | app.py:230 | app_fit: metrics_centralized {}
DEBUG flwr 2023-12-18 07:20:40,647 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:40,648 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:40,695 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:40,695 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:40,728 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:40,728 | app.py:304 | Disconnect and shut down
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
DEBUG flwr 2023-12-18 07:20:40,803 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:40,851 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,023 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,023 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,178 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,178 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,211 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,211 | app.py:304 | Disconnect and shut down
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
DEBUG flwr 2023-12-18 07:20:41,229 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,249 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,524 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,524 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,618 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,618 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,805 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,805 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,889 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,889 | app.py:304 | Disconnect and shut down
DEBUG flwr 2023-12-18 07:20:41,908 | connection.py:141 | gRPC channel closed
INFO flwr 2023-12-18 07:20:41,909 | app.py:304 | Disconnect and shut down
Total time taken:  18983.23519897461
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
Training 1 epoch(s) w/ 8551 batches each
